---
title: "Project 1: Rush Hour"
date: 2019-02-13 17:32:00 +0900
categories: Data Science
---

### 0. Motivation
프로젝트에 관련된 다양한 기술을 이용하여, 하나의 프로젝트를 진행하고 완료해보기.


### 1. 문제 제기 - Problem      
취직하면 성남이나 판교, 경기도로 갈 확률이 매우 높음.    
왕복 2시간을 출퇴근에 사용 = 하루의 10%를 소비.


### 2. 해결 방안 - Solution     
최소 출근소요시간 -> Min rush hour를 찾아냄 (+@ 최대 출근소요시간도 고려)       

### 3. 목적지&도착지 설정 (경도 longitude, 위도 latitude)     
서울시립대 (127.0551130, 37.5834520) <---> 판교 (127.1126840, 37.3947670)

### 4. 데이터 수집 (Data Mining)     
- 1단계: 노가다, 매일매일 네이버 지도 앱을 이용하여 소요시간 측정하고 기록한다.

```
|    Date    |    Duration    |
|-----------------------------|
| 1902140119 |      19분      |
| 1902140920 |      45분      |
| 1902141207 |      36분      |
| 1902141734 |      59분      |
| 1902141808 |      66분      |
| 1902142043 |      38분      |
| 1902142257 |      26분      |
| 1902151225 |      33분      |
| 1902151948 |      39분      |
| 1902161445 |      40분      |
| 1902161758 |      41분      |
| 1902170005 |      24분      |
| 1902171151 |      30분      |
| 1902171900 |      32분      |
| 1902172207 |      28분      |
| 1902172216 |      34분      |
| 1902180846 |      43분      |

----    출장으로 일시 중지    ----

| 1903091300 |      43분      |
| 1903100022 |      24분      |
| 1903132308 |      27분      |
```


- 1단계 결과      
Minimum Rush Hour = 2월 14일 새벽 1시 19분 : 19분      
Maximum Rush Hour = 2월 14일 오후 6시 08분 : 66분

대략적으로 시간이 얼마나 걸리는지에 대한 느낌정도는 왔음.        
그러나 4일정도 수동으로 일일히 데이터를 수집해본 결과, 가성비가 너무 안좋음.        
매일 같은 시간에 데이터를 모을 수 없고, 느리고, 힘들고, 속상하고, 무엇보다 전문성이 매우 떨어짐. 굳이 컴퓨터를 쓸 필요가 있을까하는 의문.       
---> 데이터 마이닝 자동화 시스템 구축 (멋짐)


- 2단계: Python 네이버 지도 크롤링     
크롬의 개발자도구를 이용하여, Request url을 얻어서 날짜와 소요시간을 저장.

(1) https://map.naver.com/ 에서 서울시립대학교 -> 판교역 길찾기    
(2) 개발자도구 -> command + option + i 이용하여 Network 들어가고, findCarRoute.nhn function 확인    

(3) Request url = 
https://map.naver.com/spirra/findCarRoute.nhn?route=route3&output=json&result=web3&coord_type=naver&search=2&car=0&mileage=12.4&start=127.0551130%2C37.5834520%2C%EC%84%9C%EC%9A%B8%EC%8B%9C%EB%A6%BD%EB%8C%80%ED%95%99%EA%B5%90&destination=127.1126840%2C37.3947670%2C%ED%8C%90%EA%B5%90%EC%97%AD+%EC%8B%A0%EB%B6%84%EB%8B%B9%EC%84%A0&via=       
Response = {"routes":[{"summary":{"distance":정수, "duration":정수, 형태로 나오는 것 확인.

(4)Python으로 url 열어서 response 값 읽고, json으로 가져와서 dataset으로 구축    
```
#!/usr/env/bin python3

from urllib.request import urlopen
import json

url = 'https://m.map.naver.com/spirra/findCarRoute.nhn?route=route3&output=json&result=web3&coord_type=latlng&search=2&car=0&mileage=12.4&start=127.0551130%2C37.5834520%2C%EC%84%9C%EC%9A%B8%EC%8B%9C%EB%A6%BD%EB%8C%80%ED%95%99%EA%B5%90&destination=127.1126840%2C37.3947670%2C%ED%8C%90%EA%B5%90%EC%97%AD+%EC%8B%A0%EB%B6%84%EB%8B%B9%EC%84%A0'

content = urlopen(url).read()
jsonValue = json.loads(content)
dataset = jsonValue['routes'][0]['summary']
duration = dataset['duration']
durationSecToMin = duration / 60
durationSecToMin = round(durationSecToMin, 2)

#get date & time
import datetime
currentDT = datetime.datetime.now()

date = currentDT.strftime('%Y%m%d%H%M')
#date = (currentDT.year * 100000000) + (currentDT.month * 1000000) + (currentDT.day * 10000) + (currentDT.hour * 100) + currentDT.minute


#save to text file
textFile = open("dataset.txt", "a+")
textFile.write(str(date) + ' : ' + str(durationSecToMin) + '\n')
textFile.close()
```

이러면 수동으로 한번짜리 코드이므로,    
조금 더 자동화시켜서 한번 켜두면 계속 크롤링하게 만.
```
#!/usr/env/bin python3

from urllib.request import urlopen
import json
import threading

def minRushHour() :
    url = 'https://m.map.naver.com/spirra/findCarRoute.nhn?route=route3&output=json&result=web3&coord_type=latlng&search=2&car=0&mileage=12.4&start=127.0551130%2C37.5834520%2C%EC%84%9C%EC%9A%B8%EC%8B%9C%EB%A6%BD%EB%8C%80%ED%95%99%EA%B5%90&destination=127.1126840%2C37.3947670%2C%ED%8C%90%EA%B5%90%EC%97%AD+%EC%8B%A0%EB%B6%84%EB%8B%B9%EC%84%A0'

    content = urlopen(url).read()
    jsonValue = json.loads(content)
    dataset = jsonValue['routes'][0]['summary']
    duration = dataset['duration']
    durationSecToMin = duration / 60
    durationSecToMin = round(durationSecToMin, 2)

    import datetime
    currentDT = datetime.datetime.now()

    date = currentDT.strftime('%Y%m%d%H%M')
    #date = (currentDT.year * 100000000) + (currentDT.month * 1000000) + (currentDT.day * 10000) + (currentDT.hour * 100) + currentDT.minute


    textFile = open("dataset.txt", "a+")
    textFile.write(str(date) + ' : ' + str(durationSecToMin) + '\n')
    textFile.close()


end = False

def autoFunction(second = 1.0):
    global end
    if end :
        return

    minRushHour()
    threading.Timer(second, autoFunction, [second]).start()


try :
    autoFunction(60)
except :
    print('warning')
```

- 2단계 결과      
서울시립대학교 -> 판교까지의 소요시간을 1분마다 dataset으로 구축했다.
```
201904251303 : 46.7
201904251304 : 46.7
201904251305 : 46.7
201904251306 : 46.7
201904251307 : 46.7
201904251308 : 44.32
201904251309 : 44.32
201904251310 : 44.32
201904251311 : 44.32
201904251312 : 44.32
201904251313 : 44.25
201904251314 : 44.25
201904251315 : 44.25
201904251316 : 44.25
201904251317 : 44.25
201904251318 : 43.8
201904251319 : 43.8
201904251320 : 43.8
201904251321 : 43.8
201904251322 : 43.8
```

+TMI, 네이버 지도의 소요 시간 함수는 밤에 10분마다, 낮에 5분마다 업데이트된다는 사실을 확인하였다.
```
$cat dataset.txt | wc -l
4406
```
data mining의 반 정도에 성공한 것 같다. 이제부터, tmux를 이용하여 꾸준하게 data를 확보해야겠다.        
next goal : 오전 6~11 시 사이의 MinRushHour를 찾기 & Linear Regression 으로 modeling 하기.


### 자동화
minRushHour.py 파일을 항상 돌리는 것은 쓰레기 데이터 마이닝을 위한 전기소모라는 느낌이 든다.        
탄력근무제를 고려하여 7시~10시 사이로 출근 시간을 가정하고 파일을 돌려서, 도착예정시간을 txt파일로 저장하자.         

(1) 크론유틸리티, cron        
```
touch crontab_file.txt #cron file 생성
crontab -e #cron file 편집
crontab crontab_file.txt #cron file 실행
```

crontab_file.txt 파일 안에 몇가지 규칙을 포함한 명령을 적어준다.        
```
$crontab -e
#In crontab_file.txt,
0 6-11 * * 1-5 /Users/username/minRushHour.py
```
분 시 일 월 요일 file 순으로 작성.

(2) tmux    
tmux를 이용하여, 데이터 구축용 간이 서버로 사용할 수 있다.
